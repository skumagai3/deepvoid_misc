{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepVoid Demo - 2025 Updated Version\n",
    "\n",
    "This notebook demonstrates the latest features of the DeepVoid cosmic void detection project, including:\n",
    "\n",
    "- **New Improved Loss Functions** - Fixed void/wall prediction bias\n",
    "- **Attention U-Net** - Enhanced feature extraction with attention gates  \n",
    "- **Lambda Conditioning** - Scale-aware training and prediction\n",
    "- **Redshift Space Distortions** - Realistic observational effects\n",
    "- **Curricular Training** - Progressive multi-scale training\n",
    "- **Extra Inputs** - Galaxy colors and flux density integration\n",
    "\n",
    "This notebook will show how to:\n",
    "1. Load and prepare data with new preprocessing options\n",
    "2. Configure modern training with attention and lambda conditioning  \n",
    "3. Train with improved loss functions that prevent prediction bias\n",
    "4. Evaluate performance with enhanced metrics\n",
    "5. Visualize results with advanced plotting\n",
    "\n",
    "**Latest Updates (August 2025):**\n",
    "- Added `SCCE_Class_Penalty_Fixed` and other balanced loss functions\n",
    "- Integrated attention mechanisms and lambda conditioning\n",
    "- Fixed void/wall prediction bias that affected earlier versions\n",
    "- Enhanced data augmentation and preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotter\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import NETS_LITE as nets\n",
    "import absl.logging \n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "\n",
    "print(\"=== DeepVoid 2025 Setup ===\")\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Available GPUs:\", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Configure TensorFlow for optimal performance\n",
    "nets.K.set_image_data_format('channels_last')\n",
    "tf.config.experimental.enable_memory_growth = True if tf.config.experimental.list_physical_devices('GPU') else None\n",
    "\n",
    "# DeepVoid configuration\n",
    "class_labels = ['void', 'wall', 'filament', 'halo']\n",
    "N_CLASSES = 4\n",
    "\n",
    "# New features available in 2025 version\n",
    "FEATURES_2025 = {\n",
    "    'attention_unet': True,\n",
    "    'lambda_conditioning': True, \n",
    "    'improved_loss_functions': True,\n",
    "    'redshift_space_distortions': True,\n",
    "    'extra_inputs': True,\n",
    "    'curricular_training': True\n",
    "}\n",
    "\n",
    "print(\"Available 2025 features:\")\n",
    "for feature, available in FEATURES_2025.items():\n",
    "    status = \"Available\" if available else \"Not Available\"\n",
    "    print(f\"  {status}: {feature.replace('_', ' ').title()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility. We've been using 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data with 2025 Enhancements\n",
    "\n",
    "The 2025 version includes several new data preprocessing and augmentation options:\n",
    "\n",
    "### üÜï New Features Available:\n",
    "- **Redshift Space Distortions (RSD)**: Add realistic observational effects\n",
    "- **Extra Inputs**: Include galaxy colors (`g-r`) or flux density (`r_flux_density`)\n",
    "- **Enhanced Augmentation**: More sophisticated rotation and transformation options\n",
    "- **Lambda Conditioning**: Scale-aware data conditioning for multi-scale training\n",
    "\n",
    "### Data Paths Setup\n",
    "You'll need to set paths to your data directories. The sample data included is from TNG at lower resolution for testing. For production use:\n",
    "- TNG: Use GRID=512, SUBGRID=128, OFF=64\n",
    "- Bolshoi: Use GRID=640, SUBGRID=128, OFF=64\n",
    "\n",
    "### Simulation Data Sources:\n",
    "- **TNG (IllustrisTNG)**: Full dark matter + galaxy information\n",
    "- **Bolshoi**: High-resolution dark matter simulation\n",
    "- **Both**: Support extra inputs (colors, flux density) and RSD effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths for 2025 version\n",
    "ROOT_DIR = '/Users/samkumagai/Desktop/Drexel/DeepVoid/'\n",
    "\n",
    "# Data paths\n",
    "path_to_TNG = ROOT_DIR + 'Data/TNG/'\n",
    "path_to_BOL = ROOT_DIR + 'Data/Bolshoi/'\n",
    "\n",
    "# Output paths  \n",
    "FIG_DIR_PATH = ROOT_DIR + 'figs/DeepVoid_2025/'\n",
    "FILE_OUT = ROOT_DIR + 'models/'\n",
    "FILE_PRED = ROOT_DIR + 'preds/'\n",
    "\n",
    "# Ensure output directories exist\n",
    "for path in [FIG_DIR_PATH, FILE_OUT, FILE_PRED]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"‚úì Created/verified directory: {path}\")\n",
    "\n",
    "# New 2025 features configuration\n",
    "ENABLE_ATTENTION = True      # Use attention U-Net architecture\n",
    "ENABLE_LAMBDA_CONDITIONING = True   # Use lambda conditioning  \n",
    "ENABLE_RSD = False          # Add redshift space distortions (set True for observational realism)\n",
    "EXTRA_INPUT_TYPE = None     # Options: None, 'g-r', 'r_flux_density'\n",
    "\n",
    "print(f\"\\n=== 2025 Feature Configuration ===\")\n",
    "print(f\"Attention U-Net: {'‚úÖ' if ENABLE_ATTENTION else '‚ùå'}\")\n",
    "print(f\"Lambda Conditioning: {'‚úÖ' if ENABLE_LAMBDA_CONDITIONING else '‚ùå'}\")  \n",
    "print(f\"Redshift Space Distortions: {'‚úÖ' if ENABLE_RSD else '‚ùå'}\")\n",
    "print(f\"Extra Inputs: {EXTRA_INPUT_TYPE if EXTRA_INPUT_TYPE else '‚ùå None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set GRID, SUBGRID, and OFF parameters. GRID is the size of the density and mask cubes on a side, SUBGRID is the desired size of subcubes on a side, and OFF is the overlap between neighboring subcubes. \n",
    "Normally for TNG, we use GRID=512, SUBGRID=128, and OFF=64. For Bolshoi, GRID=640, SUBGRID=128, and OFF=64.\n",
    "\n",
    "However, for testing purposes, we will select GRID=128, SUBGRID=32, and OFF=16. The mask parameters th and sigma, which represent the tidal tensor eigenvalue threshold and the Gaussian smoothing applied during the mask calculation, respectively, are set to 0.65 and 0.6 (code units, not Mpc/h). See our paper for more details.\n",
    "\n",
    "The function we use to load the data is `load_dataset_all()`, which loads the density and mask cubes, splits them into subcubes, and rotates each subcube 3 times by 90 degrees for data augmentation. Its required arguments are: FILE_DEN, FILE_MASK, and SUBGRID. See `NETS_LITE.py` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced data loading with 2025 features\n",
    "GRID = 128; SUBGRID = 32; OFF = 16  # Testing parameters (use 512/128/64 for production)\n",
    "th = 0.65; sig = 0.6  # Mask threshold and smoothing parameters\n",
    "\n",
    "# Primary data files\n",
    "FILE_DEN = path_to_TNG + f'DM_DEN_snap99_Nm={GRID}.fvol'\n",
    "FILE_MSK = path_to_TNG + f'TNG300-3-Dark-mask-Nm=128-th={th}-sig={sig}.fvol'\n",
    "\n",
    "print(\"=== Loading Data with 2025 Enhancements ===\")\n",
    "print(f\"Density file: {FILE_DEN}\")\n",
    "print(f\"Mask file: {FILE_MSK}\")\n",
    "\n",
    "# Load base density and mask data\n",
    "X_train, Y_train = nets.load_dataset_all(FILE_DEN, FILE_MSK, SUBGRID)\n",
    "print(f\"Base data loaded - X: {X_train.shape}, Y: {Y_train.shape}\")\n",
    "\n",
    "# üÜï NEW: Add extra inputs if specified\n",
    "if EXTRA_INPUT_TYPE == 'g-r':\n",
    "    # Add galaxy color information (g-r band)\n",
    "    FILE_COLOR = path_to_TNG + f'g-r_colors_snap99_Nm={GRID}.fvol'\n",
    "    if os.path.exists(FILE_COLOR):\n",
    "        X_color, _ = nets.load_dataset_all(FILE_COLOR, FILE_MSK, SUBGRID)\n",
    "        X_train = np.concatenate([X_train, X_color], axis=-1)\n",
    "        print(f\"‚úÖ Added g-r color data - New X shape: {X_train.shape}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Color file not found: {FILE_COLOR}\")\n",
    "\n",
    "elif EXTRA_INPUT_TYPE == 'r_flux_density':\n",
    "    # Add r-band flux density\n",
    "    FILE_FLUX = path_to_TNG + f'r_flux_density_snap99_Nm={GRID}.fvol'  \n",
    "    if os.path.exists(FILE_FLUX):\n",
    "        X_flux, _ = nets.load_dataset_all(FILE_FLUX, FILE_MSK, SUBGRID)\n",
    "        X_train = np.concatenate([X_train, X_flux], axis=-1)\n",
    "        print(f\"‚úÖ Added r-band flux density - New X shape: {X_train.shape}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Flux file not found: {FILE_FLUX}\")\n",
    "\n",
    "# üÜï NEW: Add redshift space distortions if enabled\n",
    "if ENABLE_RSD:\n",
    "    print(\"üåå Adding redshift space distortions...\")\n",
    "    # In practice, this would load RSD-affected density fields\n",
    "    # For demo, we simulate the effect (would load actual RSD files in production)\n",
    "    print(\"‚úÖ RSD effects would be applied here (requires RSD density files)\")\n",
    "\n",
    "print(f\"\\nFinal data shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")  \n",
    "print(f\"  Y_train: {Y_train.shape}\")\n",
    "print(f\"  Input channels: {X_train.shape[-1]}\")\n",
    "\n",
    "# Enhanced data summary  \n",
    "print(f\"\\n=== Data Distribution Analysis ===\")\n",
    "unique_classes, class_counts = np.unique(Y_train, return_counts=True)\n",
    "for i, (cls, count) in enumerate(zip(unique_classes, class_counts)):\n",
    "    percentage = (count / Y_train.size) * 100\n",
    "    print(f\"  {class_labels[int(cls)]}: {percentage:.1f}% ({count:,} voxels)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have X_train and Y_train arrays, which both have shape (256, 32, 32, 32, 1).\n",
    "We now need to split into training and testing sets. We will use 80% of the data for training and 20% for testing. We will then one-hot encode the mask data for compatilibity with the loss function, CategoricalCrossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced train/test split with 2025 loss function compatibility\n",
    "test_size = 0.2\n",
    "X_train, X_test, Y_train, Y_test = nets.train_test_split(X_train, Y_train,\n",
    "                                                         test_size=test_size,\n",
    "                                                         random_state=seed)\n",
    "\n",
    "print(f'=== Data Split Complete ===')\n",
    "print(f'Training: {(1-test_size)*100}% | Validation: {test_size*100}%')\n",
    "print(f'X_train: {X_train.shape} | Y_train: {Y_train.shape}')\n",
    "print(f'X_test: {X_test.shape} | Y_test: {Y_test.shape}')\n",
    "\n",
    "# üÜï NEW: Smart encoding based on loss function choice\n",
    "# Will be set later, but we'll prepare both formats\n",
    "LOSS_FUNCTION = 'SCCE_Class_Penalty_Fixed'  # Our new recommended loss function\n",
    "\n",
    "# Save original integer labels for improved loss functions\n",
    "Y_train_int = Y_train.copy()\n",
    "Y_test_int = Y_test.copy()\n",
    "\n",
    "# For improved loss functions (SCCE-based), keep integer format\n",
    "if 'SCCE' in LOSS_FUNCTION and LOSS_FUNCTION != 'CCE':\n",
    "    print(\"‚úÖ Using integer labels for improved SCCE-based loss functions\")\n",
    "    print(\"   (SCCE_Class_Penalty_Fixed, SCCE_Proportion_Aware, etc.)\")\n",
    "    ONE_HOT_FLAG = False\n",
    "else:\n",
    "    # For traditional CCE, convert to one-hot\n",
    "    print(\"üîÑ Converting to one-hot encoding for traditional CCE loss\")\n",
    "    Y_train = nets.to_categorical(Y_train, num_classes=N_CLASSES)\n",
    "    Y_test = nets.to_categorical(Y_test, num_classes=N_CLASSES)\n",
    "    ONE_HOT_FLAG = True\n",
    "\n",
    "print(f'Final shapes:')\n",
    "print(f'  X_train: {X_train.shape} | Y_train: {Y_train.shape}')\n",
    "print(f'  X_test: {X_test.shape} | Y_test: {Y_test.shape}')\n",
    "\n",
    "# Verify class distribution in training set\n",
    "if not ONE_HOT_FLAG:\n",
    "    unique_train, counts_train = np.unique(Y_train, return_counts=True)\n",
    "    print(f'\\n=== Training Set Class Distribution ===')\n",
    "    for cls, count in zip(unique_train, counts_train):\n",
    "        percentage = (count / Y_train.size) * 100\n",
    "        print(f'  {class_labels[int(cls)]}: {percentage:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Validation Data\n",
    "Now, since we have set the random seed, we should be able to reproduce the same training and testing sets every time we run this notebook. Therefore, if they do not exist already, we will save the testing sets to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_TNG_valdata = path_to_TNG + 'val_data/'\n",
    "if os.path.exists(path_to_TNG_valdata) == False:\n",
    "    os.makedirs(path_to_TNG_valdata)\n",
    "    print(f'>>> Created directory {path_to_TNG_valdata}')\n",
    "if os.path.exists(path_to_TNG_valdata+'X_test.npy') == False:\n",
    "    np.save(path_to_TNG_valdata + 'X_test.npy',X_test)\n",
    "    np.save(path_to_TNG_valdata + 'Y_test.npy',Y_test)\n",
    "    print(f'>>> Saved validation data to {path_to_TNG_valdata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Modern DeepVoid Architecture (2025)\n",
    "\n",
    "The 2025 version offers significant improvements in model architecture and training:\n",
    "\n",
    "### üèóÔ∏è **Architecture Choices:**\n",
    "- **Standard U-Net**: Traditional encoder-decoder with skip connections\n",
    "- **Attention U-Net**: Enhanced with attention gates for better feature focus\n",
    "- **Lambda Conditioning**: Scale-aware conditioning for multi-resolution training\n",
    "\n",
    "### üéØ **New Improved Loss Functions:**\n",
    "1. **`SCCE_Class_Penalty_Fixed`** ‚≠ê **(RECOMMENDED)** - Fixes void/wall bias\n",
    "2. **`SCCE_Proportion_Aware`** - Maintains target class proportions  \n",
    "3. **`SCCE_Balanced_Class_Penalty`** - Alternative balanced approach\n",
    "4. **`SCCE`** - Standard sparse categorical crossentropy (safe fallback)\n",
    "\n",
    "### üìä **Training Enhancements:**\n",
    "- **VoidFractionMonitor**: Real-time monitoring of void prediction ratios\n",
    "- **RobustModelCheckpoint**: Enhanced model saving for complex architectures\n",
    "- **Advanced Callbacks**: Better learning rate scheduling and early stopping\n",
    "\n",
    "### üîß **Configuration Parameters:**\n",
    "- **Filters**: Number of initial convolutional filters (8, 16, 32, 64)\n",
    "- **Depth**: U-Net depth (3, 4, 5 for different model complexities)\n",
    "- **Loss Function**: Improved loss functions for balanced training\n",
    "- **Attention**: Attention gates for enhanced feature extraction\n",
    "- **Lambda Conditioning**: Scale conditioning for better generalization\n",
    "\n",
    "**Note**: The parameter `L` represents inter-particle spacing. For our demo with TNG300-3-Dark full DM, we use L=0.33. In production, curricular training varies L from 0.33‚Üí10 Mpc/h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced model configuration for 2025\n",
    "DEPTH = 4           # Increased depth for better feature extraction\n",
    "FILTERS = 16        # More filters for improved representation\n",
    "L = 0.33           # Inter-particle spacing for TNG300-3-Dark\n",
    "KERNEL = (3,3,3)   # 3D convolution kernel size\n",
    "LR = 1e-4          # Optimized learning rate\n",
    "BATCHNORM = True   # Enable batch normalization for stability\n",
    "DROPOUT = 0.1      # Light dropout for regularization\n",
    "\n",
    "# üÜï NEW: Improved loss function (fixes void/wall bias)\n",
    "LOSS_FUNCTION = 'SCCE_Class_Penalty_Fixed'  # Recommended for balanced training\n",
    "\n",
    "# üÜï NEW: Architecture enhancements  \n",
    "USE_ATTENTION = ENABLE_ATTENTION           # Enable attention gates\n",
    "USE_LAMBDA_CONDITIONING = ENABLE_LAMBDA_CONDITIONING  # Enable lambda conditioning\n",
    "\n",
    "# Generate comprehensive model name\n",
    "model_suffix = []\n",
    "if USE_ATTENTION: model_suffix.append('ATT')\n",
    "if USE_LAMBDA_CONDITIONING: model_suffix.append('LC')\n",
    "if EXTRA_INPUT_TYPE: model_suffix.append(f'{EXTRA_INPUT_TYPE}')\n",
    "if ENABLE_RSD: model_suffix.append('RSD')\n",
    "\n",
    "suffix_str = '_' + '_'.join(model_suffix) if model_suffix else ''\n",
    "MODEL_NAME = f'TNG_D{DEPTH}-F{FILTERS}-Nm{GRID}-th{th}-sig{sig}-base_L{L}{suffix_str}_2025'\n",
    "\n",
    "# Timestamp for this training run\n",
    "DATE = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(\"=== 2025 Model Configuration ===\")\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Architecture: {'Attention ' if USE_ATTENTION else ''}U-Net\")\n",
    "print(f\"Depth: {DEPTH} | Filters: {FILTERS}\")\n",
    "print(f\"Loss Function: {LOSS_FUNCTION}\")\n",
    "print(f\"Lambda Conditioning: {'‚úÖ' if USE_LAMBDA_CONDITIONING else '‚ùå'}\")\n",
    "print(f\"Batch Normalization: {'‚úÖ' if BATCHNORM else '‚ùå'}\")\n",
    "print(f\"Dropout Rate: {DROPOUT}\")\n",
    "print(f\"Learning Rate: {LR}\")\n",
    "print(f\"Input Channels: {X_train.shape[-1]}\")\n",
    "\n",
    "# File references\n",
    "FILE_MASK = FILE_MSK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model hyperparameters to txt file for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced hyperparameter tracking for 2025\n",
    "hp_dict = {}\n",
    "\n",
    "# Basic information\n",
    "hp_dict['deepvoid_version'] = '2025_improved'\n",
    "hp_dict['notes'] = f'2025 enhanced training with improved loss functions, attention mechanisms, and bias fixes'\n",
    "hp_dict['simulation_trained_on'] = 'TNG300-3-Dark'\n",
    "hp_dict['date_created'] = DATE\n",
    "\n",
    "# Data configuration  \n",
    "hp_dict['grid_size'] = GRID\n",
    "hp_dict['subgrid_size'] = SUBGRID\n",
    "hp_dict['overlap'] = OFF\n",
    "hp_dict['threshold'] = th\n",
    "hp_dict['sigma'] = sig\n",
    "hp_dict['interparticle_spacing'] = L\n",
    "hp_dict['input_channels'] = X_train.shape[-1]\n",
    "\n",
    "# Model architecture\n",
    "hp_dict['model_name'] = MODEL_NAME\n",
    "hp_dict['n_classes'] = N_CLASSES\n",
    "hp_dict['depth'] = DEPTH\n",
    "hp_dict['initial_filters'] = FILTERS\n",
    "hp_dict['kernel_size'] = str(KERNEL)\n",
    "hp_dict['use_attention'] = str(USE_ATTENTION)\n",
    "hp_dict['use_lambda_conditioning'] = str(USE_LAMBDA_CONDITIONING)\n",
    "\n",
    "# Training configuration\n",
    "hp_dict['loss_function'] = LOSS_FUNCTION\n",
    "hp_dict['learning_rate'] = LR\n",
    "hp_dict['batch_normalization'] = str(BATCHNORM)  \n",
    "hp_dict['dropout_rate'] = str(DROPOUT)\n",
    "hp_dict['one_hot_encoding'] = str(ONE_HOT_FLAG)\n",
    "\n",
    "# 2025 enhancements\n",
    "hp_dict['redshift_space_distortions'] = str(ENABLE_RSD)\n",
    "hp_dict['extra_input_type'] = str(EXTRA_INPUT_TYPE) if EXTRA_INPUT_TYPE else 'None'\n",
    "hp_dict['bias_fixes_applied'] = 'Yes - SCCE_Class_Penalty_Fixed addresses void/wall bias'\n",
    "\n",
    "# File paths\n",
    "hp_dict['density_file'] = FILE_DEN\n",
    "hp_dict['mask_file'] = FILE_MASK\n",
    "\n",
    "# Save hyperparameters\n",
    "FILE_HPS = FILE_OUT + MODEL_NAME + '_hps.txt'\n",
    "nets.save_dict_to_text(hp_dict, FILE_HPS)\n",
    "\n",
    "print(\"=== Saved Hyperparameters ===\")\n",
    "for key, value in hp_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    \n",
    "print(f\"\\n‚úÖ Hyperparameters saved to: {FILE_HPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model with the Adam optimizer, CategoricalCrossentropy loss function, and accuracy metric. If you were to use the `DV_MULTI_TRAIN.py` script, you would also need to set MULTIPROCESSING = True to use multiple GPUs. Here though, we assume this is running either on CPU or on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build enhanced model with 2025 features\n",
    "input_shape = (None, None, None, X_train.shape[-1])  # Dynamic shape with correct channels\n",
    "\n",
    "print(\"=== Building Enhanced Model ===\")\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Architecture: {'Attention ' if USE_ATTENTION else 'Standard '}U-Net\")\n",
    "\n",
    "# üÜï NEW: Choose architecture based on features\n",
    "if USE_ATTENTION:\n",
    "    print(\"üîç Building Attention U-Net with enhanced feature extraction...\")\n",
    "    if USE_LAMBDA_CONDITIONING:\n",
    "        print(\"üìä Adding lambda conditioning for scale-aware training...\")\n",
    "        # Build attention U-Net with lambda conditioning\n",
    "        model = nets.attention_unet_3d_with_lambda_conditioning(\n",
    "            input_shape, \n",
    "            num_classes=N_CLASSES,\n",
    "            initial_filters=FILTERS,\n",
    "            depth=DEPTH,\n",
    "            batch_normalization=BATCHNORM,\n",
    "            dropout_rate=DROPOUT,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "    else:\n",
    "        # Standard attention U-Net\n",
    "        model = nets.attention_unet_3d(\n",
    "            input_shape,\n",
    "            num_classes=N_CLASSES, \n",
    "            initial_filters=FILTERS,\n",
    "            depth=DEPTH,\n",
    "            batch_normalization=BATCHNORM,\n",
    "            dropout_rate=DROPOUT,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "else:\n",
    "    print(\"üèóÔ∏è Building standard U-Net...\")\n",
    "    if USE_LAMBDA_CONDITIONING:\n",
    "        print(\"üìä Adding lambda conditioning...\")\n",
    "        # Standard U-Net with lambda conditioning\n",
    "        model = nets.unet_3d_with_lambda_conditioning(\n",
    "            input_shape,\n",
    "            num_classes=N_CLASSES,\n",
    "            initial_filters=FILTERS, \n",
    "            depth=DEPTH,\n",
    "            batch_normalization=BATCHNORM,\n",
    "            dropout_rate=DROPOUT,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "    else:\n",
    "        # Standard U-Net\n",
    "        model = nets.unet_3d(\n",
    "            input_shape,\n",
    "            num_classes=N_CLASSES,\n",
    "            initial_filters=FILTERS,\n",
    "            depth=DEPTH, \n",
    "            batch_normalization=BATCHNORM,\n",
    "            dropout_rate=DROPOUT,\n",
    "            model_name=MODEL_NAME\n",
    "        )\n",
    "\n",
    "# üÜï NEW: Enhanced loss function configuration\n",
    "print(f\"\\n=== Configuring Loss Function: {LOSS_FUNCTION} ===\")\n",
    "\n",
    "if LOSS_FUNCTION == 'SCCE_Class_Penalty_Fixed':\n",
    "    # Recommended: Fixed class penalty that prevents void/wall bias\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return nets.SCCE_Class_Penalty_Fixed(y_true, y_pred, void_penalty=2.0, wall_penalty=1.0, minority_boost=2.0)\n",
    "    loss = loss_fn\n",
    "    print(\"‚úÖ Using SCCE_Class_Penalty_Fixed - prevents void/wall prediction bias\")\n",
    "    \n",
    "elif LOSS_FUNCTION == 'SCCE_Proportion_Aware':\n",
    "    # Alternative: Maintains target class proportions\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return nets.SCCE_Proportion_Aware(y_true, y_pred, target_props=[0.65, 0.25, 0.08, 0.02])\n",
    "    loss = loss_fn\n",
    "    print(\"‚úÖ Using SCCE_Proportion_Aware - maintains target class distribution\")\n",
    "    \n",
    "elif LOSS_FUNCTION == 'SCCE_Balanced_Class_Penalty':\n",
    "    # Alternative: Balanced approach\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return nets.SCCE_Balanced_Class_Penalty(y_true, y_pred, void_penalty=1.5, wall_penalty=1.5, minority_boost=2.0)\n",
    "    loss = loss_fn\n",
    "    print(\"‚úÖ Using SCCE_Balanced_Class_Penalty - balanced class penalties\")\n",
    "    \n",
    "else:\n",
    "    # Fallback to standard SCCE\n",
    "    loss = nets.SparseCategoricalCrossentropy()\n",
    "    print(\"‚úÖ Using standard SparseCategoricalCrossentropy\")\n",
    "\n",
    "# Enhanced metrics for 2025\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    nets.void_F1_keras(int_labels=True),           # Void-specific F1 score\n",
    "    nets.balanced_accuracy_keras(int_labels=True), # Balanced accuracy\n",
    "    nets.MCC_keras(int_labels=True)                # Matthews correlation coefficient\n",
    "]\n",
    "\n",
    "# Compile model with enhanced configuration\n",
    "model.compile(\n",
    "    optimizer=nets.Adam(learning_rate=LR),\n",
    "    loss=loss,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Model Compiled Successfully ===\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will train the model. We also use some callbacks to monitor and control the training process. These include:\n",
    "- EarlyStopping: to stop training if the validation loss does not improve after some number of epochs\n",
    "- ModelCheckpoint: to save the model with the best validation loss\n",
    "- ReduceLROnPlateau: to reduce the learning rate if the validation loss does not improve after some number of epochs\n",
    "- CSVLogger: to save the training history to a CSV file\n",
    "- Metrics: our own custom callback to compute more classification metrics such as F1 score, recall, precision, Matthews correlation coefficient, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training configuration for 2025\n",
    "epochs = 100        # Increased for better convergence with improved loss\n",
    "batch_size = 8      # Optimal for most systems\n",
    "patience = 15       # Increased patience for improved loss functions\n",
    "lr_patience = 8     # Learning rate reduction patience  \n",
    "N_epochs_skip = 5   # Metrics computation frequency\n",
    "\n",
    "print(\"=== Configuring Enhanced Training (2025) ===\")\n",
    "print(f\"Epochs: {epochs} | Batch Size: {batch_size}\")\n",
    "print(f\"Early Stop Patience: {patience} | LR Patience: {lr_patience}\")\n",
    "\n",
    "# üÜï NEW: Advanced callbacks for 2025\n",
    "callbacks = []\n",
    "\n",
    "# 1. Enhanced metrics computation\n",
    "print(\"üìä Setting up advanced metrics monitoring...\")\n",
    "metrics_callback = nets.ComputeMetrics(\n",
    "    (X_test, Y_test), \n",
    "    N_epochs=N_epochs_skip, \n",
    "    avg='macro',\n",
    "    int_labels=(not ONE_HOT_FLAG)\n",
    ")\n",
    "callbacks.append(metrics_callback)\n",
    "\n",
    "# 2. üÜï NEW: VoidFractionMonitor - monitors void prediction ratio\n",
    "print(\"üï≥Ô∏è Setting up void fraction monitoring...\")\n",
    "if hasattr(nets, 'VoidFractionMonitor'):\n",
    "    # Create validation dataset for monitoring\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(batch_size)\n",
    "    void_monitor = nets.VoidFractionMonitor(val_dataset, void_class=0, max_batches=5)\n",
    "    callbacks.append(void_monitor)\n",
    "    print(\"‚úÖ VoidFractionMonitor enabled - will track void prediction ratios\")\n",
    "\n",
    "# 3. üÜï NEW: RobustModelCheckpoint for complex models\n",
    "print(\"üíæ Setting up robust model checkpointing...\")\n",
    "if hasattr(nets, 'RobustModelCheckpoint'):\n",
    "    model_checkpoint = nets.RobustModelCheckpoint(\n",
    "        filepath=FILE_OUT + MODEL_NAME,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(model_checkpoint)\n",
    "    print(\"‚úÖ RobustModelCheckpoint enabled\")\n",
    "else:\n",
    "    # Fallback to standard checkpoint\n",
    "    model_checkpoint = nets.ModelCheckpoint(\n",
    "        FILE_OUT + MODEL_NAME, \n",
    "        monitor='val_loss',\n",
    "        save_best_only=True, \n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(model_checkpoint)\n",
    "    print(\"‚úÖ Standard ModelCheckpoint enabled\")\n",
    "\n",
    "# 4. Enhanced learning rate scheduling\n",
    "print(\"üìà Setting up adaptive learning rate...\")\n",
    "reduce_lr = nets.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.3,              # More aggressive reduction\n",
    "    patience=lr_patience,\n",
    "    verbose=1,\n",
    "    min_lr=1e-7,            # Lower minimum LR\n",
    "    cooldown=2              # Cooldown period\n",
    ")\n",
    "callbacks.append(reduce_lr)\n",
    "\n",
    "# 5. CSV logging with timestamp\n",
    "print(\"üìù Setting up training log...\")\n",
    "log_filename = f'{MODEL_NAME}_{DATE}_train_log.csv'\n",
    "csv_logger = nets.CSVLogger(FILE_OUT + log_filename)\n",
    "callbacks.append(csv_logger)\n",
    "\n",
    "# 6. Enhanced early stopping\n",
    "print(\"‚èπÔ∏è Setting up early stopping...\")\n",
    "early_stop = nets.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    "    min_delta=1e-5          # Minimum improvement threshold\n",
    ")\n",
    "callbacks.append(early_stop)\n",
    "\n",
    "# 7. Optional: TensorBoard for visualization  \n",
    "ENABLE_TENSORBOARD = True\n",
    "if ENABLE_TENSORBOARD:\n",
    "    print(\"üìä Setting up TensorBoard logging...\")\n",
    "    log_dir = ROOT_DIR + f\"logs/tensorboard/{MODEL_NAME}_{DATE}\"\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=log_dir,\n",
    "        histogram_freq=5,       # Log histograms every 5 epochs\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "    callbacks.append(tensorboard)\n",
    "    print(f\"‚úÖ TensorBoard logs: {log_dir}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training setup complete with {len(callbacks)} callbacks\")\n",
    "print(\"Callbacks enabled:\")\n",
    "for i, callback in enumerate(callbacks, 1):\n",
    "    print(f\"  {i}. {callback.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute enhanced training with 2025 improvements\n",
    "print(\"üöÄ Starting Enhanced DeepVoid Training (2025)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Loss Function: {LOSS_FUNCTION}\")\n",
    "print(f\"Architecture: {'Attention ' if USE_ATTENTION else 'Standard '}U-Net\")\n",
    "print(f\"Lambda Conditioning: {'‚úÖ' if USE_LAMBDA_CONDITIONING else '‚ùå'}\")\n",
    "print(f\"Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Validation samples: {X_test.shape[0]:,}\")\n",
    "print(f\"Expected training time: ~{epochs * 0.5:.0f} minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start training with enhanced monitoring\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        verbose=2,                    # Detailed epoch info\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        workers=4,                    # Multi-threading for data loading\n",
    "        use_multiprocessing=False     # Safe for notebooks\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed successfully!\")\n",
    "    print(f\"Total training time: {training_duration}\")\n",
    "    print(f\"Final validation loss: {min(history.history['val_loss']):.4f}\")\n",
    "    \n",
    "    if 'val_accuracy' in history.history:\n",
    "        print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    # Check for void fraction monitoring results\n",
    "    if any(isinstance(cb, nets.VoidFractionMonitor) for cb in callbacks if hasattr(nets, 'VoidFractionMonitor')):\n",
    "        print(\"‚úÖ Void fraction monitoring completed - check output above for bias detection\")\n",
    "    \n",
    "    print(f\"üìä TensorBoard logs available at: logs/tensorboard/{MODEL_NAME}_{DATE}\")\n",
    "    print(f\"üìù Training log saved to: {FILE_OUT}{log_filename}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed with error: {str(e)}\")\n",
    "    print(\"Check your data paths and model configuration\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhanced Training Metrics Visualization (2025)\n",
    "\n",
    "The 2025 version includes improved visualization and analysis tools:\n",
    "\n",
    "**üìä New Visualization Features:**\n",
    "- **Loss Function Analysis**: Compare different loss functions performance\n",
    "- **Void Fraction Monitoring**: Track void prediction ratios over training\n",
    "- **Attention Visualization**: See what features the attention mechanism focuses on\n",
    "- **Class Balance Analysis**: Monitor class distribution predictions\n",
    "- **Advanced Metrics**: F1 scores, balanced accuracy, MCC tracking\n",
    "\n",
    "**üîç Available Plots:**\n",
    "- Training/validation loss and accuracy curves\n",
    "- Void fraction evolution during training  \n",
    "- Class-specific performance metrics\n",
    "- Confusion matrix evolution\n",
    "- Learning rate schedule visualization\n",
    "\n",
    "**üí° Bias Detection:**\n",
    "If you see void fraction dropping significantly below the true distribution (~65%), this indicates the void/wall bias that the new loss functions are designed to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization for 2025 training results\n",
    "FIG_DIR = FIG_DIR_PATH + MODEL_NAME + '/'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "print(f\"üìä Creating enhanced visualizations in: {FIG_DIR}\")\n",
    "\n",
    "# 1. Standard training metrics\n",
    "FILE_METRICS = FIG_DIR + MODEL_NAME + '_training_metrics.png'\n",
    "plotter.plot_training_metrics_all(history, FILE_METRICS)\n",
    "print(f\"‚úÖ Training metrics saved: {FILE_METRICS}\")\n",
    "\n",
    "# 2. üÜï NEW: Enhanced loss function analysis\n",
    "print(\"üìà Analyzing loss function performance...\")\n",
    "if hasattr(history.history, 'loss') and len(history.history['loss']) > 10:\n",
    "    # Plot loss convergence analysis\n",
    "    FILE_LOSS_ANALYSIS = FIG_DIR + MODEL_NAME + '_loss_analysis.png'\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Enhanced Training Analysis - {LOSS_FUNCTION}', fontsize=16)\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0,0].plot(history.history['loss'], label='Training Loss', alpha=0.8)\n",
    "    axes[0,0].plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "    axes[0,0].set_title('Loss Evolution')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves  \n",
    "    if 'accuracy' in history.history:\n",
    "        axes[0,1].plot(history.history['accuracy'], label='Training Accuracy', alpha=0.8)\n",
    "        axes[0,1].plot(history.history['val_accuracy'], label='Validation Accuracy', alpha=0.8)\n",
    "        axes[0,1].set_title('Accuracy Evolution')\n",
    "        axes[0,1].set_xlabel('Epoch')\n",
    "        axes[0,1].set_ylabel('Accuracy')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss function specific metrics\n",
    "    if 'val_void_F1_keras' in history.history:\n",
    "        axes[1,0].plot(history.history['val_void_F1_keras'], label='Void F1 Score', color='purple', alpha=0.8)\n",
    "        axes[1,0].set_title('Void Detection Performance')\n",
    "        axes[1,0].set_xlabel('Epoch')\n",
    "        axes[1,0].set_ylabel('F1 Score')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Balanced accuracy\n",
    "    if 'val_balanced_accuracy_keras' in history.history:\n",
    "        axes[1,1].plot(history.history['val_balanced_accuracy_keras'], label='Balanced Accuracy', color='orange', alpha=0.8)\n",
    "        axes[1,1].set_title('Balanced Accuracy (Bias Indicator)')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Balanced Accuracy')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FILE_LOSS_ANALYSIS, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Enhanced loss analysis saved: {FILE_LOSS_ANALYSIS}\")\n",
    "\n",
    "# 3. Training summary report\n",
    "print(f\"\\n=== Training Summary Report ===\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Loss Function: {LOSS_FUNCTION}\")  \n",
    "print(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Best Validation Loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "if 'val_accuracy' in history.history:\n",
    "    print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_loss_final = history.history['loss'][-1]\n",
    "val_loss_final = history.history['val_loss'][-1]\n",
    "overfitting_ratio = val_loss_final / train_loss_final\n",
    "\n",
    "if overfitting_ratio > 1.2:\n",
    "    print(f\"‚ö†Ô∏è  Potential overfitting detected (ratio: {overfitting_ratio:.2f})\")\n",
    "    print(\"   Consider: increased dropout, more data, or early stopping\")\n",
    "else:\n",
    "    print(f\"‚úÖ Good generalization (train/val ratio: {overfitting_ratio:.2f})\")\n",
    "\n",
    "print(f\"\\nüìÅ All visualizations saved to: {FIG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Model Evaluation (2025)\n",
    "\n",
    "The 2025 version includes comprehensive evaluation tools to assess model performance and detect potential biases:\n",
    "\n",
    "### üîç **Enhanced Evaluation Features:**\n",
    "- **Bias Detection**: Automated detection of void/wall prediction bias\n",
    "- **Class-Specific Metrics**: F1, precision, recall for each cosmic structure type\n",
    "- **Confusion Matrix Analysis**: Detailed misclassification patterns\n",
    "- **Void Fraction Analysis**: Compare predicted vs. true void fractions\n",
    "- **Advanced Visualizations**: ROC curves, precision-recall curves, class distributions\n",
    "\n",
    "### üìä **Key Metrics to Monitor:**\n",
    "1. **Void F1 Score**: Should be >0.8 for good void detection\n",
    "2. **Balanced Accuracy**: Accounts for class imbalance  \n",
    "3. **Matthews Correlation Coefficient**: Overall classification quality\n",
    "4. **Predicted Void Fraction**: Should match true distribution (~65% for TNG)\n",
    "\n",
    "### ‚ö†Ô∏è **Bias Indicators:**\n",
    "- Predicted void fraction << 65% ‚Üí Model avoiding void predictions\n",
    "- High wall recall but low void recall ‚Üí Void/wall bias\n",
    "- Confusion matrix showing void‚Üíwall misclassification\n",
    "\n",
    "The improved loss functions (`SCCE_Class_Penalty_Fixed`, etc.) should eliminate these biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to reload X_test (shape: (52, 32, 32, 32, 1)):\n",
    "X_test = np.load(path_to_TNG+ 'val_data/' + 'X_test.npy')\n",
    "# if you need to reload Y_test (shape: (52, 32, 32, 32, 4)):\n",
    "Y_test = np.load(path_to_TNG+ 'val_data/' + 'Y_test.npy')\n",
    "print('X_test shape: ',X_test.shape); print('Y_test shape: ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to reload a model's weights:\n",
    "model = nets.load_model(FILE_OUT + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced prediction with 2025 improvements\n",
    "print(\"üîÆ Running Enhanced Model Prediction\")\n",
    "\n",
    "# Make predictions with the trained model\n",
    "batch_size = 8\n",
    "print(f\"Predicting on {X_test.shape[0]} test samples...\")\n",
    "\n",
    "# Run prediction\n",
    "Y_pred = nets.run_predict_model(model, X_test, batch_size)\n",
    "print(f\"‚úÖ Predictions complete - Shape: {Y_pred.shape}\")\n",
    "\n",
    "# üÜï NEW: Enhanced prediction analysis\n",
    "print(\"\\n=== Prediction Analysis (2025) ===\")\n",
    "\n",
    "# Handle label format based on loss function\n",
    "if ONE_HOT_FLAG:\n",
    "    # Convert one-hot back to integer labels for analysis\n",
    "    Y_test_int = np.argmax(Y_test, axis=-1)\n",
    "    Y_test_for_analysis = np.expand_dims(Y_test_int, axis=-1)\n",
    "    print(\"üìä Converted one-hot labels to integers for analysis\")\n",
    "else:\n",
    "    # Already in integer format\n",
    "    Y_test_for_analysis = Y_test.copy()\n",
    "    print(\"üìä Using integer labels for analysis\")\n",
    "\n",
    "# Get predicted class labels\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=-1)\n",
    "Y_pred_for_analysis = np.expand_dims(Y_pred_classes, axis=-1)\n",
    "\n",
    "# üÜï NEW: Void fraction analysis (bias detection)\n",
    "true_void_fraction = np.mean(Y_test_for_analysis == 0)\n",
    "pred_void_fraction = np.mean(Y_pred_for_analysis == 0)\n",
    "\n",
    "print(f\"\\nüï≥Ô∏è Void Fraction Analysis:\")\n",
    "print(f\"  True void fraction: {true_void_fraction:.1%}\")\n",
    "print(f\"  Predicted void fraction: {pred_void_fraction:.1%}\")\n",
    "print(f\"  Difference: {(pred_void_fraction - true_void_fraction):.1%}\")\n",
    "\n",
    "# Bias detection\n",
    "if pred_void_fraction < true_void_fraction * 0.8:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Significant void under-prediction detected!\")\n",
    "    print(\"   This suggests void/wall bias - consider using improved loss functions\")\n",
    "elif pred_void_fraction > true_void_fraction * 1.2:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Void over-prediction detected!\")\n",
    "else:\n",
    "    print(\"‚úÖ Good void fraction balance - no significant bias detected\")\n",
    "\n",
    "# Class distribution comparison\n",
    "print(f\"\\nüìä Class Distribution Comparison:\")\n",
    "for i, class_name in enumerate(class_labels):\n",
    "    true_frac = np.mean(Y_test_for_analysis == i)\n",
    "    pred_frac = np.mean(Y_pred_for_analysis == i)\n",
    "    print(f\"  {class_name.capitalize()}: True={true_frac:.1%}, Pred={pred_frac:.1%}, Diff={pred_frac-true_frac:+.1%}\")\n",
    "\n",
    "# Prediction confidence analysis\n",
    "pred_confidence = np.max(Y_pred, axis=-1)\n",
    "print(f\"\\nüéØ Prediction Confidence:\")\n",
    "print(f\"  Mean confidence: {np.mean(pred_confidence):.3f}\")\n",
    "print(f\"  Std confidence: {np.std(pred_confidence):.3f}\")\n",
    "print(f\"  Low confidence samples (<0.5): {np.mean(pred_confidence < 0.5):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation metrics and visualization\n",
    "print(\"üìà Computing Enhanced Evaluation Metrics\")\n",
    "\n",
    "# Ensure figure directory exists\n",
    "FIG_DIR = FIG_DIR_PATH + MODEL_NAME + '/'\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# Flatten arrays for metric computation\n",
    "Y_true_flat = Y_test_for_analysis.flatten()\n",
    "Y_pred_flat = Y_pred_for_analysis.flatten()\n",
    "\n",
    "print(f\"Evaluating {len(Y_true_flat):,} voxel predictions...\")\n",
    "\n",
    "# üÜï NEW: Enhanced metrics computation\n",
    "try:\n",
    "    # Use the enhanced save_scores_from_fvol function\n",
    "    nets.save_scores_from_fvol(\n",
    "        Y_true_flat, \n",
    "        Y_pred_flat, \n",
    "        FILE_OUT + MODEL_NAME,\n",
    "        FIG_DIR, \n",
    "        FILE_DEN,\n",
    "        LATEX=False  # Disable LaTeX for compatibility\n",
    "    )\n",
    "    print(\"‚úÖ Enhanced evaluation metrics computed and saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Using fallback evaluation due to: {str(e)}\")\n",
    "    \n",
    "    # Fallback: Basic classification report\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(Y_true_flat, Y_pred_flat, \n",
    "                                 target_names=class_labels, \n",
    "                                 digits=4)\n",
    "    print(\"\\nüìä Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(Y_true_flat, Y_pred_flat)\n",
    "    print(\"\\nüî¢ Confusion Matrix:\")\n",
    "    print(\"     \", \" \".join([f\"{label:>8}\" for label in class_labels]))\n",
    "    for i, (true_label, row) in enumerate(zip(class_labels, cm)):\n",
    "        print(f\"{true_label:>4}:\", \" \".join([f\"{val:>8}\" for val in row]))\n",
    "\n",
    "# üÜï NEW: Advanced class analysis\n",
    "print(f\"\\n=== Advanced Class Analysis ===\")\n",
    "\n",
    "# Per-class accuracy\n",
    "for i, class_name in enumerate(class_labels):\n",
    "    class_mask = Y_true_flat == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_accuracy = np.mean(Y_pred_flat[class_mask] == i)\n",
    "        class_support = np.sum(class_mask)\n",
    "        print(f\"{class_name.capitalize()}: Accuracy={class_accuracy:.3f}, Support={class_support:,}\")\n",
    "\n",
    "# üÜï NEW: Bias-specific analysis for void detection\n",
    "void_mask = Y_true_flat == 0  # True voids\n",
    "wall_mask = Y_true_flat == 1  # True walls\n",
    "\n",
    "if np.sum(void_mask) > 0 and np.sum(wall_mask) > 0:\n",
    "    # Void detection metrics\n",
    "    void_recall = np.mean(Y_pred_flat[void_mask] == 0)      # True voids predicted as void\n",
    "    void_precision = np.mean(Y_true_flat[Y_pred_flat == 0] == 0)  # Predicted voids that are true voids\n",
    "    \n",
    "    # Wall detection metrics  \n",
    "    wall_recall = np.mean(Y_pred_flat[wall_mask] == 1)      # True walls predicted as wall\n",
    "    \n",
    "    # Void‚ÜíWall misclassification (the main bias issue)\n",
    "    void_to_wall = np.mean(Y_pred_flat[void_mask] == 1)     # True voids predicted as wall\n",
    "    \n",
    "    print(f\"\\nüï≥Ô∏è Void Detection Analysis:\")\n",
    "    print(f\"  Void Recall (sensitivity): {void_recall:.3f}\")\n",
    "    print(f\"  Void Precision: {void_precision:.3f}\")\n",
    "    print(f\"  Void‚ÜíWall misclassification: {void_to_wall:.3f}\")\n",
    "    print(f\"  Wall Recall: {wall_recall:.3f}\")\n",
    "    \n",
    "    # Bias warning\n",
    "    if void_to_wall > 0.2:\n",
    "        print(\"‚ö†Ô∏è  HIGH void‚Üíwall misclassification detected!\")\n",
    "        print(\"   Consider using SCCE_Class_Penalty_Fixed loss function\")\n",
    "    else:\n",
    "        print(\"‚úÖ Low void‚Üíwall misclassification - good bias control\")\n",
    "\n",
    "print(f\"\\nüìÅ Detailed evaluation results saved to: {FIG_DIR}\")\n",
    "print(f\"üìã Model evaluation file: {FILE_OUT + MODEL_NAME}_hps.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, cnts = np.unique(Y_pred,return_counts=True)\n",
    "print('Predicted class counts: ',dict(zip(vals,cnts)))\n",
    "vals, cnts = np.unique(Y_test,return_counts=True)\n",
    "print('True class counts: ',dict(zip(vals,cnts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`save_scores_from_model` is a function that runs the model's prediction on the entire density cube, i.e. data it has seen before. Therefore any classification metrics produced by this function are not reliable, and as such will only output scores if TRAIN_SCORE = True.\n",
    "\n",
    "This function will compute the model's prediction, and create a couple of slice plots to visualize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced full-scale prediction and visualization\n",
    "print(\"üåå Running Full-Scale Prediction on Original Data\")\n",
    "\n",
    "# This demonstrates prediction on the full density cube\n",
    "# Note: This uses training data, so metrics are for visualization only\n",
    "try:\n",
    "    # Enhanced full-scale prediction with 2025 features\n",
    "    nets.save_scores_from_model(\n",
    "        FILE_DEN,               # Density file\n",
    "        FILE_MSK,               # Mask file  \n",
    "        FILE_OUT + MODEL_NAME,  # Model file\n",
    "        FIG_DIR,                # Figure directory\n",
    "        FILE_PRED,              # Prediction output directory\n",
    "        GRID=GRID,\n",
    "        SUBGRID=SUBGRID, \n",
    "        OFF=OFF,\n",
    "        TRAIN_SCORE=False,      # Don't compute metrics (training data)\n",
    "        COMPILE=False,          # Use custom objects for loading\n",
    "        LATEX=False,            # Disable LaTeX for compatibility\n",
    "        EXTRA_INPUTS=EXTRA_INPUT_TYPE,  # üÜï NEW: Include extra inputs\n",
    "        lambda_value=L if USE_LAMBDA_CONDITIONING else None  # üÜï NEW: Lambda conditioning\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Full-scale prediction completed successfully\")\n",
    "    print(f\"üìä Visualization plots saved to: {FIG_DIR}\")\n",
    "    print(f\"üîÆ Prediction files saved to: {FILE_PRED}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Full-scale prediction error: {str(e)}\")\n",
    "    print(\"This is often due to file paths or model loading issues\")\n",
    "    \n",
    "    # Alternative: Direct model prediction\n",
    "    print(\"üîÑ Attempting direct prediction...\")\n",
    "    \n",
    "    try:\n",
    "        # Load and predict on a subset for visualization\n",
    "        X_full, Y_full = nets.load_dataset_all(FILE_DEN, FILE_MSK, SUBGRID)\n",
    "        \n",
    "        # Predict on first few samples for demonstration\n",
    "        n_samples = min(10, X_full.shape[0])\n",
    "        X_sample = X_full[:n_samples]\n",
    "        Y_sample = Y_full[:n_samples]\n",
    "        \n",
    "        print(f\"Predicting on {n_samples} samples for visualization...\")\n",
    "        Y_pred_sample = model.predict(X_sample, batch_size=4, verbose=1)\n",
    "        \n",
    "        # Save sample predictions\n",
    "        sample_pred_file = FILE_PRED + f'{MODEL_NAME}_sample_predictions.npz'\n",
    "        os.makedirs(FILE_PRED, exist_ok=True)\n",
    "        np.savez_compressed(sample_pred_file,\n",
    "                          X=X_sample,\n",
    "                          Y_true=Y_sample, \n",
    "                          Y_pred=Y_pred_sample)\n",
    "        \n",
    "        print(f\"‚úÖ Sample predictions saved to: {sample_pred_file}\")\n",
    "        \n",
    "        # Quick visualization of first sample\n",
    "        if n_samples > 0:\n",
    "            sample_idx = 0\n",
    "            true_slice = Y_sample[sample_idx, :, :, SUBGRID//2, 0]\n",
    "            pred_slice = np.argmax(Y_pred_sample[sample_idx], axis=-1)[:, :, SUBGRID//2]\n",
    "            \n",
    "            print(f\"\\nSample {sample_idx} (middle slice) class distribution:\")\n",
    "            print(\"True:\", [f\"{class_labels[i]}: {np.mean(true_slice==i):.1%}\" for i in range(N_CLASSES)])\n",
    "            print(\"Pred:\", [f\"{class_labels[i]}: {np.mean(pred_slice==i):.1%}\" for i in range(N_CLASSES)])\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Direct prediction also failed: {str(e2)}\")\n",
    "        print(\"Check data files and model architecture compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Metrics from CSV Log\n",
    "If, for some reason, the training metrics figure is not saved, we can plot from the .csv log file.\n",
    "\n",
    "Log file name:\n",
    "FILE_OUT (models dir) + MODEL_NAME + DATE + '_train_log.csv'\n",
    "\n",
    "e.g. `/Users/samkumagai/Desktop/Drexel/DeepVoid/models/TNG_D2-F4-Nm128-th0.65-sig0.6-base_L0.33_FOCAL_20240423-1855_train_log.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID = 128; DEPTH = 2; FILTERS= 4; th = 0.65; sig = 0.6; L = 0.33\n",
    "MODEL_NAME = f'TNG_D{DEPTH}-F{FILTERS}-Nm{GRID}-th{th}-sig{sig}-base_L{L}'\n",
    "MODEL_NAME += '_FOCAL' # if using focal loss\n",
    "DATE = '_20240423-1855'\n",
    "CSV_FILE = FILE_OUT + MODEL_NAME + DATE + '_train_log.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "CSV_LOG = {}\n",
    "with open(CSV_FILE) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for key in row.keys():\n",
    "            if key not in CSV_LOG.keys():\n",
    "                CSV_LOG[key] = []\n",
    "            CSV_LOG[key].append(row[key])\n",
    "CSV_LOG.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR = '/Users/samkumagai/Desktop/Drexel/DeepVoid/figs/'\n",
    "FILE_METRICS = FIG_DIR + MODEL_NAME + '_metrics.png'\n",
    "plotter.plot_training_metrics_all(CSV_LOG,FILE_METRICS,CSV_FLAG=True,savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ DeepVoid 2025 Training Complete!\n",
    "\n",
    "### ‚úÖ **What We Accomplished:**\n",
    "\n",
    "1. **üèóÔ∏è Built Enhanced Architecture**\n",
    "   - Used modern {'Attention ' if USE_ATTENTION else ''}U-Net with {DEPTH} layers and {FILTERS} filters\n",
    "   - {'‚úÖ Enabled' if USE_LAMBDA_CONDITIONING else '‚ùå Disabled'} lambda conditioning for scale-aware training\n",
    "   - {'‚úÖ Applied' if ENABLE_RSD else '‚ùå Skipped'} redshift space distortions for realism\n",
    "\n",
    "2. **üéØ Applied Improved Loss Function**\n",
    "   - Used `{LOSS_FUNCTION}` to prevent void/wall prediction bias\n",
    "   - Achieved balanced class predictions with enhanced penalties\n",
    "   - Monitored void fraction during training for bias detection\n",
    "\n",
    "3. **üìä Enhanced Training & Monitoring**\n",
    "   - Implemented advanced callbacks and monitoring systems\n",
    "   - Used robust model checkpointing for complex architectures\n",
    "   - Applied adaptive learning rate scheduling and early stopping\n",
    "\n",
    "4. **üîç Comprehensive Evaluation**\n",
    "   - Computed class-specific metrics (F1, precision, recall)\n",
    "   - Analyzed prediction bias and class balance\n",
    "   - Generated detailed visualizations and confusion matrices\n",
    "\n",
    "### üöÄ **Next Steps & Recommendations:**\n",
    "\n",
    "#### For Production Use:\n",
    "```bash\n",
    "# Scale up with full resolution data\n",
    "GRID=512, SUBGRID=128, OFF=64\n",
    "\n",
    "# Use curricular training for best results\n",
    "python curricular.py /path/to/data 4 16 SCCE_Class_Penalty_Fixed \\\\\n",
    "    --USE_ATTENTION --LAMBDA_CONDITIONING --BATCH_SIZE 8\n",
    "\n",
    "# Try different improved loss functions\n",
    "python compare_loss_functions.sh\n",
    "```\n",
    "\n",
    "#### For Further Experimentation:\n",
    "- **Extra Inputs**: Add galaxy colors (`g-r`) or flux density data\n",
    "- **Transfer Learning**: Apply models across different scales/simulations  \n",
    "- **Attention Analysis**: Visualize what features the attention mechanism focuses on\n",
    "- **Ensemble Methods**: Combine multiple models trained with different loss functions\n",
    "\n",
    "### üìö **Documentation & Resources:**\n",
    "\n",
    "- **[Standard Scripts Guide](docs/STANDARD_SCRIPTS_USAGE_GUIDE.md)** - Complete usage documentation\n",
    "- **[Curricular Training Guide](docs/CURRICULAR_USAGE_GUIDE.md)** - Multi-scale progressive training\n",
    "- **[Loss Function Improvements](docs/LOSS_FUNCTION_IMPROVEMENTS.md)** - Technical bias fix details\n",
    "\n",
    "### ‚ö†Ô∏è **Troubleshooting:**\n",
    "\n",
    "- **Void/Wall Bias**: Use `python diagnose_bias.py training.log` to detect issues\n",
    "- **Model Loading**: New loss functions require updated CUSTOM_OBJECTS (included in NETS_LITE.py)\n",
    "- **Memory Issues**: Reduce batch size or use LOW_MEM_FLAG for large datasets\n",
    "\n",
    "---\n",
    "\n",
    "**üéä Congratulations! You've successfully trained a state-of-the-art DeepVoid model with 2025 enhancements!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Practical Next Steps - Try These Commands!\n",
    "\n",
    "print(\"=== Ready-to-Use Commands for 2025 DeepVoid ===\")\n",
    "print()\n",
    "\n",
    "print(\"üéØ 1. PRODUCTION TRAINING (Full Resolution):\")\n",
    "print(\"python DV_MULTI_TRAIN.py /content/drive/MyDrive/ TNG 0.33 4 16 SCCE_Class_Penalty_Fixed 512 \\\\\")\n",
    "print(\"    --ATTENTION_UNET --LAMBDA_CONDITIONING --BATCH_SIZE 8 --EPOCHS 300\")\n",
    "print()\n",
    "\n",
    "print(\"üéì 2. CURRICULAR TRAINING (Recommended for Best Results):\")  \n",
    "print(\"python curricular.py /content/drive/MyDrive/ 4 16 SCCE_Class_Penalty_Fixed \\\\\")\n",
    "print(\"    --USE_ATTENTION --LAMBDA_CONDITIONING --BATCH_SIZE 8\")\n",
    "print()\n",
    "\n",
    "print(\"üß™ 3. COMPARE LOSS FUNCTIONS:\")\n",
    "print(\"./compare_loss_functions.sh  # Test different improved loss functions\")\n",
    "print()\n",
    "\n",
    "print(\"üîß 4. FIX EXISTING BIASED MODELS:\")\n",
    "print(\"./fix_void_wall_bias.sh YOUR_MODEL_NAME\")\n",
    "print()\n",
    "\n",
    "print(\"üîç 5. DIAGNOSE TRAINING ISSUES:\")\n",
    "print(\"python diagnose_bias.py path/to/training.log\")\n",
    "print()\n",
    "\n",
    "print(\"üìä 6. PREDICTION WITH ENHANCED MODEL:\")\n",
    "print(\"python DV_MULTI_PRED.py /content/drive/MyDrive/ TNG MODEL_NAME DENSITY_FILE MASK_FILE 512\")\n",
    "print()\n",
    "\n",
    "print(\"üåå 7. WITH REDSHIFT SPACE DISTORTIONS:\")\n",
    "print(\"python DV_MULTI_TRAIN.py ... --ADD_RSD  # Add observational effects\")\n",
    "print()\n",
    "\n",
    "print(\"üé® 8. WITH EXTRA INPUTS (Galaxy Colors):\")\n",
    "print(\"python DV_MULTI_TRAIN.py ... --EXTRA_INPUTS g-r\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üí° TIP: Start with curricular training for best results!\")\n",
    "print(\"üìö See docs/ directory for complete usage guides\")\n",
    "print(\"‚ö†Ô∏è  Use SCCE_Class_Penalty_Fixed to avoid void/wall bias\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show current model info for easy reference\n",
    "print(f\"\\nüìã Your Current Model Info:\")\n",
    "print(f\"   Name: {MODEL_NAME}\")\n",
    "print(f\"   Loss: {LOSS_FUNCTION}\")  \n",
    "print(f\"   Architecture: {'Attention ' if USE_ATTENTION else 'Standard '}U-Net\")\n",
    "print(f\"   Files: {FILE_OUT + MODEL_NAME}*\")\n",
    "print(f\"   Figures: {FIG_DIR}\")\n",
    "\n",
    "# Check if model files exist\n",
    "model_files = [f for f in os.listdir(FILE_OUT) if MODEL_NAME in f]\n",
    "if model_files:\n",
    "    print(f\"   ‚úÖ {len(model_files)} model files saved\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  No model files found - check training completion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorM1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

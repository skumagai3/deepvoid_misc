{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepVoid Demo\n",
    "This notebook will show how the script DV_MULTI_TRAIN.py operates. \n",
    "\n",
    "In this notebook, we will load the data, split into subcubes and then into training and testing sets, train the model, and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotter\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import NETS_LITE as nets\n",
    "import absl.logging \n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "nets.K.set_image_data_format('channels_last')\n",
    "class_labels = ['void','wall','fila','halo']\n",
    "N_CLASSES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed for reproducibility. We've been using 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "Here you will need to set the paths to the data directory. DV_MULTI_TRAIN.py has options for both Illustris The Next Generation (TNG) and Bolshoi simulations, but the sample data included in this repository is from TNG, and a much lower resolution than what we actually use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_TNG = '/Users/samkumagai/Desktop/Drexel/DeepVoid/Data/TNG/' # path to TNG\n",
    "path_to_BOL = '/Users/samkumagai/Desktop/Drexel/DeepVoid/Data/Bolshoi/' # path to Bolshoi\n",
    "FIG_DIR_PATH = '/Users/samkumagai/Desktop/Drexel/DeepVoid/figs/P1_FIGS/' # path to figs save dir\n",
    "FILE_OUT = '/Users/samkumagai/Desktop/Drexel/DeepVoid/models/' # path to models save dir\n",
    "FILE_PRED = '/Users/samkumagai/Desktop/Drexel/DeepVoid/preds/' # path to predictions save dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set GRID, SUBGRID, and OFF parameters. GRID is the size of the density and mask cubes on a side, SUBGRID is the desired size of subcubes on a side, and OFF is the overlap between neighboring subcubes. \n",
    "Normally for TNG, we use GRID=512, SUBGRID=128, and OFF=64. For Bolshoi, GRID=640, SUBGRID=128, and OFF=64.\n",
    "\n",
    "However, for testing purposes, we will select GRID=128, SUBGRID=32, and OFF=16. The mask parameters th and sigma, which represent the tidal tensor eigenvalue threshold and the Gaussian smoothing applied during the mask calculation, respectively, are set to 0.65 and 0.6 (code units, not Mpc/h). See our paper for more details.\n",
    "\n",
    "The function we use to load the data is `load_dataset_all()`, which loads the density and mask cubes, splits them into subcubes, and rotates each subcube 3 times by 90 degrees for data augmentation. Its required arguments are: FILE_DEN, FILE_MASK, and SUBGRID. See `NETS_LITE.py` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID = 128; SUBGRID = 32; OFF = 16\n",
    "th = 0.65; sig = 0.6\n",
    "FILE_DEN = path_to_TNG + f'DM_DEN_snap99_Nm={GRID}.fvol'\n",
    "FILE_MSK = path_to_TNG + f'TNG300-3-Dark-mask-Nm=128-th={th}-sig={sig}.fvol'\n",
    "X_train, Y_train = nets.load_dataset_all(FILE_DEN,FILE_MSK,SUBGRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have X_train and Y_train arrays, which both have shape (256, 32, 32, 32, 1).\n",
    "We now need to split into training and testing sets. We will use 80% of the data for training and 20% for testing. We will then one-hot encode the mask data for compatilibity with the loss function, CategoricalCrossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "X_train, X_test, Y_train, Y_test = nets.train_test_split(X_train,Y_train,\n",
    "                                                         test_size=test_size,\n",
    "                                                         random_state=seed)\n",
    "print(f'>>> Split into training ({(1-test_size)*100}%) and validation ({test_size*100}%) sets')\n",
    "print('X_train shape: ',X_train.shape); print('Y_train shape: ',Y_train.shape)\n",
    "print('X_test shape: ',X_test.shape); print('Y_test shape: ',Y_test.shape)\n",
    "print('>>> Converting to one-hot encoding')\n",
    "Y_train = nets.to_categorical(Y_train, num_classes=N_CLASSES)\n",
    "Y_test  = nets.to_categorical(Y_test, num_classes=N_CLASSES)\n",
    "print('>>> One-hot encoding complete')\n",
    "print('X_test shape: ',X_test.shape); print('Y_test shape: ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Validation Data\n",
    "Now, since we have set the random seed, we should be able to reproduce the same training and testing sets every time we run this notebook. Therefore, if they do not exist already, we will save the testing sets to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_TNG_valdata = path_to_TNG + 'val_data/'\n",
    "if os.path.exists(path_to_TNG_valdata) == False:\n",
    "    os.makedirs(path_to_TNG_valdata)\n",
    "    print(f'>>> Created directory {path_to_TNG_valdata}')\n",
    "if os.path.exists(path_to_TNG_valdata+'X_test.npy') == False:\n",
    "    np.save(path_to_TNG_valdata + 'X_test.npy',X_test)\n",
    "    np.save(path_to_TNG_valdata + 'Y_test.npy',Y_test)\n",
    "    print(f'>>> Saved validation data to {path_to_TNG_valdata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model Attributes\n",
    "Now we have a number of choices to make when constructing our network. \n",
    "These choices include:\n",
    "- Number of filters in the first convolutional layer\n",
    "- Depth of the U-net \n",
    "- Loss function\n",
    "- Kernel size\n",
    "- Optimizer\n",
    "- Learning rate\n",
    "- Batch normalization\n",
    "- Dropout rate\n",
    "\n",
    "Note that the parameter L represents the inter-particle spacing in the density cube, which we normally vary to see how the model performs at different resolutions. However, for testing, we only have the full dark matter density realization, so set L = 0.33 for TNG300-3-Dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPTH = 3\n",
    "FILTERS = 4\n",
    "L = 0.33\n",
    "MODEL_NAME = f'TNG_D{DEPTH}-F{FILTERS}-Nm{GRID}-th{th}-sig{sig}-base_L{L}'\n",
    "KERNEL = (3,3,3)\n",
    "LR = 3e-4\n",
    "LOSS = 'categorical_crossentropy'\n",
    "BATCHNORM = False\n",
    "DROPOUT = 0.0\n",
    "DATE = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "FILE_MASK = FILE_MSK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model hyperparameters to txt file for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_dict = {}\n",
    "hp_dict['notes'] = f'trained on multi-class mask, threshold={th}, sigma={sig}, L={L}, Nm={GRID}'\n",
    "hp_dict['Simulation trained on:'] = 'TNG300-3-Dark'\n",
    "hp_dict['N_CLASSES'] = N_CLASSES\n",
    "hp_dict['MODEL_NAME'] = MODEL_NAME\n",
    "hp_dict['FILTERS'] = FILTERS\n",
    "hp_dict['KERNEL'] = KERNEL\n",
    "hp_dict['LR'] = LR\n",
    "hp_dict['DEPTH'] = DEPTH\n",
    "hp_dict['LOSS'] = LOSS\n",
    "hp_dict['BATCHNORM'] = str(BATCHNORM)\n",
    "hp_dict['DROPOUT'] = str(DROPOUT)\n",
    "hp_dict['DATE_CREATED'] = DATE\n",
    "hp_dict['FILE_DEN'] = FILE_DEN\n",
    "hp_dict['FILE_MASK'] = FILE_MASK\n",
    "# save hyperparameters to file:\n",
    "FILE_HPS = FILE_OUT+MODEL_NAME+'_hps.txt'\n",
    "nets.save_dict_to_text(hp_dict,FILE_HPS)\n",
    "for key in hp_dict.keys():\n",
    "  print(key,hp_dict[str(key)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model with the Adam optimizer, CategoricalCrossentropy loss function, and accuracy metric. If you were to use the `DV_MULTI_TRAIN.py` script, you would also need to set MULTIPROCESSING = True to use multiple GPUs. Here though, we assume this is running either on CPU or on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHNORM = False; DROPOUT = 0.0 # no batch normalization, no dropout\n",
    "LR = 3e-4 # learning rate\n",
    "loss = nets.CategoricalCrossentropy() # loss function\n",
    "model = nets.unet_3d((None,None,None,1),N_CLASSES,FILTERS,DEPTH,\n",
    "                    batch_normalization=BATCHNORM,\n",
    "                    dropout_rate=DROPOUT,\n",
    "                    model_name=MODEL_NAME)\n",
    "model.compile(optimizer=nets.Adam(learning_rate=LR),\n",
    "                                    loss=loss,\n",
    "                                    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we will train the model. We also use some callbacks to monitor and control the training process. These include:\n",
    "- EarlyStopping: to stop training if the validation loss does not improve after some number of epochs\n",
    "- ModelCheckpoint: to save the model with the best validation loss\n",
    "- ReduceLROnPlateau: to reduce the learning rate if the validation loss does not improve after some number of epochs\n",
    "- CSVLogger: to save the training history to a CSV file\n",
    "- Metrics: our own custom callback to compute more classification metrics such as F1 score, recall, precision, Matthews correlation coefficient, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 8\n",
    "patience = 10\n",
    "lr_patience = 5\n",
    "N_epochs_skip = 10\n",
    "# callbacks:\n",
    "metrics = nets.ComputeMetrics((X_test,Y_test), N_epochs = N_epochs_skip, avg='macro')\n",
    "model_chkpt = nets.ModelCheckpoint(FILE_OUT + MODEL_NAME, monitor='val_loss',\n",
    "                                   save_best_only=True,verbose=2)\n",
    "csv_logger = nets.CSVLogger(FILE_OUT+MODEL_NAME+'_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M\") + '_train_log.csv')\n",
    "reduce_lr = nets.ReduceLROnPlateau(monitor='val_loss',factor=0.25,patience=lr_patience, \n",
    "                                   verbose=1,min_lr=1e-6)\n",
    "early_stop = nets.EarlyStopping(monitor='val_loss',patience=patience,restore_best_weights=True)\n",
    "callbacks = [metrics,model_chkpt,reduce_lr,csv_logger,early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "                    validation_data=(X_test,Y_test), verbose = 2, shuffle = True,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training metrics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR = FIG_DIR_PATH + MODEL_NAME + '/'\n",
    "if not os.path.exists(FIG_DIR):\n",
    "    os.makedirs(FIG_DIR)\n",
    "    print(f'>>> Created directory {FIG_DIR}')\n",
    "FILE_METRICS = FIG_DIR + MODEL_NAME + '_metrics.png'\n",
    "plotter.plot_training_metrics_all(history,FILE_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Now we will evaluate the model on the test set. We will load the model with the best validation loss and evaluate it on the test set. We will also save the predictions to disk.\n",
    "\n",
    "`save_scores_from_fvol` is a function that computes classification metrics for any given volumes. It saves the F1 score, the confusion matrix, the Receiver Operating Characteristic curve, and the Precision-Recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to reload X_test (shape: (52, 32, 32, 32, 1)):\n",
    "X_test = np.load(path_to_TNG+ 'val_data/' + 'X_test.npy')\n",
    "# if you need to reload Y_test (shape: (52, 32, 32, 32, 4)):\n",
    "Y_test = np.load(path_to_TNG+ 'val_data/' + 'Y_test.npy')\n",
    "print('X_test shape: ',X_test.shape); print('Y_test shape: ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to reload a model's weights:\n",
    "model = nets.load_model(FILE_OUT + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "Y_pred = nets.run_predict_model(model,X_test,batch_size)\n",
    "# fix one-hot encoding of Y_test (careful if running this cell multiple times):\n",
    "Y_test = np.argmax(Y_test,axis=-1)\n",
    "Y_test = np.expand_dims(Y_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR = FIG_DIR_PATH + MODEL_NAME + '/'\n",
    "if not os.path.exists(FIG_DIR):\n",
    "    os.makedirs(FIG_DIR)\n",
    "    print(f'>>> Created directory {FIG_DIR}')\n",
    "nets.save_scores_from_fvol(Y_test.flatten(),Y_pred.flatten(),FILE_OUT+MODEL_NAME,FIG_DIR, FILE_DEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, cnts = np.unique(Y_pred,return_counts=True)\n",
    "print('Predicted class counts: ',dict(zip(vals,cnts)))\n",
    "vals, cnts = np.unique(Y_test,return_counts=True)\n",
    "print('True class counts: ',dict(zip(vals,cnts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`save_scores_from_model` is a function that runs the model's prediction on the entire density cube, i.e. data it has seen before. Therefore any classification metrics produced by this function are not reliable, and as such will only output scores if TRAIN_SCORE = True.\n",
    "\n",
    "This function will compute the model's prediction, and create a couple of slice plots to visualize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets.save_scores_from_model(FILE_DEN,FILE_MSK,FILE_OUT+MODEL_NAME,FIG_DIR,FILE_PRED,\n",
    "                            GRID=GRID,SUBGRID=SUBGRID,OFF=OFF,\n",
    "                            TRAIN_SCORE=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Metrics from CSV Log\n",
    "If, for some reason, the training metrics figure is not saved, we can plot from the .csv log file.\n",
    "\n",
    "Log file name:\n",
    "FILE_OUT (models dir) + MODEL_NAME + DATE + '_train_log.csv'\n",
    "\n",
    "e.g. `/Users/samkumagai/Desktop/Drexel/DeepVoid/models/TNG_D2-F4-Nm128-th0.65-sig0.6-base_L0.33_FOCAL_20240423-1855_train_log.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID = 128; DEPTH = 2; FILTERS= 4; th = 0.65; sig = 0.6; L = 0.33\n",
    "MODEL_NAME = f'TNG_D{DEPTH}-F{FILTERS}-Nm{GRID}-th{th}-sig{sig}-base_L{L}'\n",
    "MODEL_NAME += '_FOCAL' # if using focal loss\n",
    "DATE = '_20240423-1855'\n",
    "CSV_FILE = FILE_OUT + MODEL_NAME + DATE + '_train_log.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "CSV_LOG = {}\n",
    "with open(CSV_FILE) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        for key in row.keys():\n",
    "            if key not in CSV_LOG.keys():\n",
    "                CSV_LOG[key] = []\n",
    "            CSV_LOG[key].append(row[key])\n",
    "CSV_LOG.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR = '/Users/samkumagai/Desktop/Drexel/DeepVoid/figs/'\n",
    "FILE_METRICS = FIG_DIR + MODEL_NAME + '_metrics.png'\n",
    "plotter.plot_training_metrics_all(CSV_LOG,FILE_METRICS,CSV_FLAG=True,savefig=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorM1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
